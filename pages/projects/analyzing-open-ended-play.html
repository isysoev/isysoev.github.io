<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Ivan Sysoev -> Analyzing open-ended learning play</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="../../assets/main/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" crossorigin="anonymous"></script>
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../../css/agency.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container-fluid">
                <a class="project-title" href="../../index.html#analyzing-open-ended-play">To main</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    <i class="fas fa-bars ms-1"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                        <li class="nav-item"><a class="nav-link" href="#stories">Stories from logs and observations</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- About -->
        <section class="page-section" id="about">
            <div class="container">
                <div class="row text-center">
                    <h2 class="section-heading text-uppercase">Analyzing open-ended learning play</h2>
                </div>
                <div class="row justify-content-center">
                    <div class="col-12 col-sm-10 col-md-8 col-lg-6">
                        <img class="img-fluid centerline-graphics" src="../../assets/projects/analyzing-open-ended-play/poster.jpg" alt="..." />
                    </div>
                </div>
                <div class = "row">
                    <p class = "text-center text-muted">
                        <i>MIT Media Lab, 2016-2020</i><br>
                        <i>Roles: participated in Design, Development and Research</i><br>
                        <i>Collaborators: Mina Soltangheis, Anneli Hershman. Advisor: Deb Roy</i>
                    </p>
                    <p class = "text-left"> 
                        Most of my research focuses on open-ended, playful learning. While powerful in terms of facilitating the child's engagement, agency and self-efficacy, open-ended learning is difficult to analyze because of its complexity: so many different and interesting things could happen! To support understanding of these phenomena, we collect very detailed logs on the devices where our learning apps run - the level of detail is sufficient to reconstruct the entirety of what has happened on-device. We then developed tools that can help to unearthing interesting things in this data - e.g. by providing a bird-eye view on play sessions.
                    </p>
                    <p>
                        Above, you can see a so-called play tree. This visualization was developed to examine how children <a class="inline-href" href="tinkering-with-words.html">tinker with words</a> in a dedicated literacy app. It allows to easily see how words were pulled apart and put together, and thus to quickly scan entire play sessions for interesting patterns.
                    </p>
                    <p>
                        Below is another tool developed for the same project, called <i>Play Observatory</i>. A video recording of a play session is shown in sync with reconstructions of the app screens which are generated from log files. This allows us to see what children did in context.
                    </p>
                </div>
                <div class="row justify-content-center">
                    <div class="col-12 col-md-10 col-lg-8">
                        <img class="img-fluid centerline-graphics" src="../../assets/projects/analyzing-open-ended-play/play-observatory.jpg" alt="..." />
                    </div>
                </div>
            </div>
        </section>
        <!-- Word scaffolding -->
        <section class="page-section bg-light" id="stories">
            <div class="container">
                <div class="row text-center">
                    <h2 class="section-heading text-uppercase">Stories from logs and observations</h2>
                </div>
                <div class="row">
                    <p>
                        Let's look at some stories that a combination of data analysis and observations can reveal. The play tree below shows an example of a child building a word (her name) backwards. A look at the video data shows her being puzzled why the app doesn't say her name right. The facilitators then explained the concept of word direction to her, and she was able to correct her word. This is an example of a learning moment revealed by a bird-eye view of play sessions. 
                    </p>
                </div>
                <div class="row justify-content-center">
                    <div class="col-12 col-sm-10 col-md-8 col-lg-6">
                        <div class="embed-responsive embed-responsive-16by9 centerline-graphics">
                            <iframe class="embed-responsive-item youtube" src="https://www.youtube.com/embed/9YcnZ-TSeYo" allowfullscreen></iframe>
                        </div>
                    </div>
                </div>
                <div class="row">
                    <p>
                        A sequence of images below is the timelapse of a scene created by a child playing with <a class="inline-href" href="child-driven-machine-guided.html">SpeechBlocks II</a> app. They are reconstructed from the logs. Records from the classroom tell us two stories: (1) how her ideas were partially initiated by her, partially prompted by the app itself, and (2) how she used various input systems to make the words she wanted:
                    </p>
                </div>
                <div class="row justify-content-center">
                    <div class="col-12 col-sm-10">
                        <p>
                            At first, the child built two ninjas and said, <i>“They are father and son. They are practicing”</i>. She then expressed a desire to give them weapons and used invented spelling recognition to create SOD (sword). Then she resorted to speech recognition to build SHIELD. Afterwards, she tapped on the sword to see the related words, picked DAGGER and gave it to the small ninja. This was followed by a long exploration of the semantic association network, until she stumbled upon the word PRISONER. This discovery prompted her to exclaim, <i>“I’m going to make a villain to fight them!”</i>, which led to the complete scene.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <p>
                        This is an example of how qualitative observations and log data can be combined to tell a richer story.
                    </p>
                </div>
                <div class="row justify-content-center">
                    <div class="col-12 col-md-10 col-lg-8">
                        <img class="img-fluid centerline-graphics" src="../../assets/projects/analyzing-open-ended-play/timeline-of-a-scene-construction.jpg" alt="..." />
                    </div>
                </div>
                <div class="row">
                    <p>
                        Sometimes quantitative analysis can complement the picture drawn from qualitative observations. In the analysis of play with <a class="inline-href" href="child-driven-machine-guided.html">SpeechBlocks II</a> app, we noticed that children with lower self-regulation (specifically, executive functioning, EF) and literacy skills (specifically, phonological awareness, or PA) were likely to engage in distracted behaviors, such as random tapping and swiping and "taking pictures" using the word recognition interface. We looked at a few quantitative measures that we thought to be plausibly associated with these behaviors, such as number of touches per session (to capture quick tapping) and speed and "jerkiness" of finger movements (to capture quick random swipes). We saw that, indeed, all of them increased with low PA and EF, and sometimes the association was strong enough to be statistically significant. It corroborated our impression that the app was not sufficiently supporing the children with lower self-regulation and literacy skills. You can find more about this study in our <a class="inline-href" href="../../assets/publications/Sysoev-et-al-CompEdu-Child-Driven-Machine-Guided.pdf">Computers & Education paper</a>.
                    </p>
                </div>
                <div class="row justify-content-center">
                    <div class="col-12 col-md-10 col-lg-8">
                        <img class="img-fluid centerline-graphics" src="../../assets/projects/analyzing-open-ended-play/impulsive-interaction-measures.jpg" alt="..." />
                        <p class="text-muted" style="margin-top: -2em; font-size: 0.7em; text-align: right">
                            Table from Sysoev et. al. (2022) Child-driven, machine-guided: Automatic scaffolding of constructionist-inspired early literacy play
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="../../js/agency.js"></script>
    </body>
</html>
